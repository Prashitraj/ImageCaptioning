{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImageCaptioning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj-zYss4OfDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import pickle\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--fSPxwDBne2",
        "colab_type": "text"
      },
      "source": [
        "# CNN Encoder\n",
        "It encodes the input image into a 1D vector. The input image is of size 224*224.There are three convolutional and one fully connected linear layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHqcLH_6RZ9M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderCNN(nn.Module):\n",
        "    def convs(self,x):\n",
        "            x = F.max_pool2d(F.relu(self.conv1(x)),(3,3))\n",
        "            x = F.max_pool2d(F.relu(self.conv2(x)),(3,3))\n",
        "            x = F.max_pool2d(F.relu(self.conv3(x)),(3,3))\n",
        "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
        "            # print(self._to_linear)\n",
        "            return x\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.convs(x)\n",
        "        x = x.view(-1,self._to_linear)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return x\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3,32,5)\n",
        "        self.conv2 = nn.Conv2d(32,64,5)\n",
        "        self.conv3 = nn.Conv2d(64,128,5)\n",
        "        self._to_linear = None\n",
        "        x = torch.randn(3,224,224).view(-1,3,224,224)\n",
        "        self.convs(x)\n",
        "        # print(self._to_linear)\n",
        "        self.fc1 = nn.Linear(self._to_linear,512)\n",
        "encoder = EncoderCNN()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmXWPJLlOi4v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self,embed_size,hidden_size,vocab_size,num_layers = 2):\n",
        "        super().__init__()\n",
        "        self.embeding_layer = nn.Embedding(vocab_size,embed_size)\n",
        "        self.lstm = nn.LSTM(input_size = embed_size,hidden_size = hidden_size,num_layers = num_layers,batch_first = True)\n",
        "        self.linear = nn.Linear(hidden_size,vocab_size)\n",
        "    \n",
        "    def forward(self,features,captions):\n",
        "        embed = self.embeding_layer(captions[:,:-1])\n",
        "        # print(features.unsqueeze(1).shape,embed.shape)\n",
        "        embed = torch.cat((features.unsqueeze(1),embed), dim = 1)\n",
        "        lstm_outputs, _ = self.lstm(embed)\n",
        "        out  = self.linear(lstm_outputs)\n",
        "        return out\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVJ5iqd5TF5Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "38936bef-3a17-4c2a-b8d1-bd0ae0bef4f5"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import csv\n",
        "import string\n",
        "\n",
        "class Vocabulary():\n",
        "        \n",
        "    def load_vocab(self,file_path,vocab_file):\n",
        "        words = []\n",
        "        \n",
        "        # nltk.download('punkt')\n",
        "        with open(file_path) as file:\n",
        "            rows = csv.reader(file, delimiter=\"\\t\")\n",
        "            for row in tqdm(rows):\n",
        "                for i in range(5):\n",
        "                    word_tokenized_list = nltk.tokenize.word_tokenize(row[i+1])\n",
        "                    word_tokenized_no_punct = [x for x in word_tokenized_list if x not in string.punctuation]\n",
        "                    words.extend(word_tokenized_no_punct)\n",
        "        words_count = Counter(words)\n",
        "        words_thresholded = [x for x in words_count if words_count[x]>10]\n",
        "        words_thresholded.append(self.start_word)\n",
        "        words_thresholded.append(self.end_word)\n",
        "        words_thresholded.sort()\n",
        "        # print(len(words_thresholded))\n",
        "        vocab = {}\n",
        "        for i, word in tqdm(enumerate(words_thresholded)):\n",
        "            vocab.update({\n",
        "                word: i\n",
        "            })\n",
        "        pickle.dump(vocab,open(vocab_file,'wb'))\n",
        "        return vocab\n",
        "    def __init__(self,train_captions = '/content/drive/My Drive/Assignment4/train_captions.tsv',\n",
        "                 word_count_threshold = 5, vocab_file = '/content/drive/My Drive/Assignment4/vocab.pk'):\n",
        "        self.vocab = None\n",
        "        self.start_word = '<start>'\n",
        "        self.end_word = '<end>'\n",
        "        if os.path.exists(vocab_file):\n",
        "            self.vocab = pickle.load(open(vocab_file,'rb'))\n",
        "            print(\"loading from logs\")\n",
        "        else:\n",
        "            self.vocab = self.load_vocab(train_captions,vocab_file)\n",
        "        print(\"generating vocabulary\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFG8BLTQWmmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word2vec(vocab,caption,maxl):\n",
        "    tokens = []\n",
        "    tokens.append(vocab.start_word)\n",
        "    caption  = nltk.tokenize.word_tokenize(caption)\n",
        "    caption = [x for x in caption if x not in string.punctuation]\n",
        "    tokens.extend(caption)\n",
        "    tokens.append(vocab.end_word)\n",
        "    out = []\n",
        "    for i,word in enumerate(tokens):\n",
        "        if word in vocab.vocab.keys():\n",
        "            out.append(vocab.vocab[word])\n",
        "    while len(out)<maxl+2:\n",
        "        out.append(vocab.vocab[vocab.end_word])\n",
        "    out = torch.Tensor(out).to(device)\n",
        "    return out "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ql-KeG6O-3Bh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "def transform(input_image_path):\n",
        "    transform  = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])])\n",
        "    image = Image.open(input_image_path)\n",
        "    out_image = transform(image)\n",
        "    return out_image"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ9KYrRpCl7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "def load_captions(captions_path = '/content/drive/My Drive/Assignment4/train_captions.tsv'):\n",
        "    captions = {}\n",
        "    indices = []\n",
        "    with open(captions_path) as file:\n",
        "        rows = csv.reader(file,delimiter = \"\\t\")\n",
        "        for row in tqdm(rows):\n",
        "            captions[row[0]] = row[1:]\n",
        "            indices.append(row[0])\n",
        "    indices = np.array(indices)\n",
        "    return indices,captions"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkrj2oqgR1HZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "54347003-2a04-42a5-f7d9-0d2d96a9a6a2"
      },
      "source": [
        "embedding_size = 512\n",
        "hidden_size = 512\n",
        "num_epochs = 5\n",
        "batch_size = 100\n",
        "vocab  = Vocabulary()\n",
        "vocab_size = len(vocab.vocab)\n",
        "encoder = EncoderCNN()\n",
        "decoder = DecoderRNN(embedding_size,hidden_size,vocab_size)\n",
        "indices,captions = load_captions()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "encoder.to(device)\n",
        "decoder.to(device)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loading from logs\n",
            "generating vocabulary\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "29000it [00:02, 13016.42it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecoderRNN(\n",
              "  (embeding_layer): Embedding(5217, 512)\n",
              "  (lstm): LSTM(512, 512, num_layers=2, batch_first=True)\n",
              "  (linear): Linear(in_features=512, out_features=5217, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hT4x1yuSG5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
        "\n",
        "params = list(decoder.parameters()) + list(encoder.parameters())\n",
        "\n",
        "optimizer = torch.optim.Adam(params = params, lr = 0.002)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCu5gmBHh5E1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_batches = int(len(indices)/batch_size)\n",
        "\n",
        "for i in tqdm(range(num_epochs)):\n",
        "    for k in tqdm(range(num_batches)):\n",
        "        train_indices = indices[k*batch_size:(k+1)*batch_size]\n",
        "        images = []\n",
        "        for image in train_indices:\n",
        "            images.append(transform(input_image_path='/content/drive/My Drive/Assignment4/train_images/train_images/image_'+image+'.jpg'))\n",
        "        images = torch.stack(images)\n",
        "        images = images.to(device)\n",
        "        for j in range(5):\n",
        "            word_vec = []\n",
        "            maxl = 0\n",
        "            for index in train_indices:\n",
        "                caption = nltk.tokenize.word_tokenize(captions[index][j])\n",
        "                maxl = max(maxl,len(caption))\n",
        "            # print(maxl)\n",
        "            for index in train_indices:\n",
        "                caption = captions[index][j]\n",
        "                word_vec.append(word2vec(vocab,caption,maxl).long())\n",
        "            word_vec = torch.stack(word_vec)\n",
        "            word_vec.to(device)\n",
        "            \n",
        "            decoder.zero_grad()\n",
        "\n",
        "            features = encoder(images)\n",
        "            outputs = decoder(features, word_vec)\n",
        "\n",
        "            \n",
        "            loss = criterion(outputs.view(-1, vocab_size), word_vec.view(-1))\n",
        "        \n",
        "        # Backward pass.\n",
        "            loss.backward()\n",
        "        \n",
        "        # Update the parameters in the optimizer.\n",
        "            optimizer.step()\n",
        "    \n",
        "    print(loss.item())\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJWhJtIhc-B3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pickle.dump(encoder,open('/content/drive/My Drive/Assignment4/encoder.pk','wb'))\n",
        "pickle.dump(decoder,open('/content/drive/My Drive/Assignment4/decoder.pk','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YStuX6bnm0F8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = pickle.load(open('/content/drive/My Drive/Assignment4/encoder.pk','rb'))\n",
        "decoder = pickle.load(open('/content/drive/My Drive/Assignment4/decoder.pk','rb'))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzbf8SkifvdC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(inputs,stop_index,states=None, max_len=30):\n",
        "        \" accepts pre-processed image tensor (inputs) and returns predicted sentence (list of tensor ids of length max_len) \"\n",
        "        output_sentence = []\n",
        "        decoder.lstm.flatten_parameters()\n",
        "        for i in range(max_len):\n",
        "            lstm_outputs, states = decoder.lstm(inputs, states)\n",
        "            lstm_outputs = lstm_outputs.squeeze(1)\n",
        "            out = decoder.linear(lstm_outputs)\n",
        "            last_pick = out.max(1)[1]\n",
        "            if int(last_pick) == int(stop_index):\n",
        "                break\n",
        "            output_sentence.append(last_pick.item())\n",
        "            inputs = decoder.embeding_layer(last_pick).unsqueeze(1)\n",
        "        return output_sentence"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM8cE8Vyrhe8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "key_list = list(vocab.vocab.keys()) \n",
        "val_list = list(vocab.vocab.values())\n",
        "test_file = []"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yS8JEX-wSO5g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7e8db7a2-6285-4847-f0dd-868136ae61e0"
      },
      "source": [
        "for path,subdir,f in os.walk('/content/drive/My Drive/Assignment4/private_test_images/private_test_images/'):\n",
        "    for im in tqdm(f):\n",
        "        image = transform(input_image_path='/content/drive/My Drive/Assignment4/private_test_images/private_test_images/'+im)\n",
        "        features = encoder.forward(image.view(-1,3,224,224).to(device))\n",
        "        features = torch.stack([features])\n",
        "        out_test = test(features,vocab.vocab[vocab.end_word])\n",
        "        out_string =  \"\"\n",
        "        out_row = []\n",
        "        out_row.append(int(im[6:-4]))\n",
        "        # print (out_string)\n",
        "        for i in range(1,len(out_test)-1):\n",
        "            out_string += key_list[val_list.index(int(out_test[i]))]+\" \"\n",
        "        out_string += key_list[val_list.index(int(out_test[len(out_test)-1]))] + \".\"\n",
        "        out_row.append(out_string)\n",
        "        # print(out_row)\n",
        "        test_file.append(out_row)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [08:04<00:00,  2.06it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXePu-0jSUmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/drive/My Drive/Assignment4/2017CS10359_NA_private.tsv', 'w',encoding='utf8') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "    for row in test_file:\n",
        "        csvwriter.writerow([str(row[0])+\"\\t\"+row[1]])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcXrOqrzTdaO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "676dc880-f3da-443f-e4b3-f66b8d7feda0"
      },
      "source": [
        "test_file = []\n",
        "for path,subdir,f in os.walk('/content/drive/My Drive/Assignment4/public_test_images/public_test_images/'):\n",
        "    for im in tqdm(f):\n",
        "        image = transform(input_image_path='/content/drive/My Drive/Assignment4/public_test_images/public_test_images/'+im)\n",
        "        features = encoder.forward(image.view(-1,3,224,224).to(device))\n",
        "        features = torch.stack([features])\n",
        "        out_test = test(features,vocab.vocab[vocab.end_word])\n",
        "        out_string =  \"\"\n",
        "        out_row = []\n",
        "        out_row.append(int(im[6:-4]))\n",
        "        # print (out_string)\n",
        "        for i in range(1,len(out_test)-1):\n",
        "            out_string += key_list[val_list.index(int(out_test[i]))]+\" \"\n",
        "        out_string += key_list[val_list.index(int(out_test[len(out_test)-1]))] + \".\"\n",
        "        out_row.append(out_string)\n",
        "        # print(out_row)\n",
        "        test_file.append(out_row)\n",
        "\n",
        "with open('/content/drive/My Drive/Assignment4/2017CS10359_NA_public.tsv', 'w',encoding='utf8') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "    for row in test_file:\n",
        "        csvwriter.writerow([str(row[0])+\"\\t\"+row[1]])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1014/1014 [08:06<00:00,  2.08it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}